{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/compomics/ML-course-VIB-2024/blob/master/notebooks/Histone_marks_dt.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiAyYSFE0YHL"
   },
   "source": [
    "# Histone modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Krri7AXy8UM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7DJU4Tny8Ua"
   },
   "source": [
    "# 1. Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alZpE70qy8Uh"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data/data_train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/ML-course-VIB-2020/master/data/data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZuSUhLu4L8x"
   },
   "outputs": [],
   "source": [
    "train_ids = train.pop(\"GeneId\")\n",
    "train_labels = train.pop(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4VjbA3ey8VN"
   },
   "outputs": [],
   "source": [
    "test_index_col = test.pop(\"GeneId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FesoIiyNy8Vd"
   },
   "source": [
    "# 2. Fitting a decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaqTeiPjy8Vg"
   },
   "source": [
    "The scikit-learn `DecisionTreeClassifier` class computes a decision tree predictive model from a dataset. \n",
    "\n",
    "To get all the options for learning you can simply type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGi4kfLzy8Vh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNn63QiNy8Vo"
   },
   "source": [
    "You notice that there are many hyperparameters to set. Some of these have a large impact on the complexity of the fitted model. An important such hyperparameter is the `max_depth` that sets a limit on how deep a decision tree can become. \n",
    "\n",
    "Let's create a decision tree model with `max_depth=3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0Pjoa08y8Vp"
   },
   "outputs": [],
   "source": [
    "cls = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-0bRK0ky8Vy"
   },
   "source": [
    "This creates a decision tree model with default values for the other hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vVO8Qnuy8V0"
   },
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYVjJV6Ay8WF"
   },
   "source": [
    "Let's create a validation set, fit the model and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPl3orbDy8WH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train,train_labels,\n",
    "                                                  test_size=.2, random_state=random_seed)\n",
    "\n",
    "cls.fit(train_X,train_y)\n",
    "\n",
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
    "\n",
    "predictions_train_prob = cls.predict_proba(train_X)\n",
    "predictions_val_prob = cls.predict_proba(val_X)\n",
    "\n",
    "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,-1]),log_loss(val_y,predictions_val_prob[:,-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZcpHy1My8Wj"
   },
   "source": [
    "The following code plots the fitted decision tree `cls` as a `tree.png` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H7Nlqs9y8Wl"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from io import StringIO\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "\n",
    "out = StringIO()\n",
    "tree.export_graphviz(cls, out_file=out)\n",
    "graph=pydotplus.graph_from_dot_data(out.getvalue())\n",
    "graph.write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUjU9oGUB6vG"
   },
   "source": [
    "How do other values for for the `max_depth` hyperparameter perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLYKBq2u4VJT"
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {'max_depth': range(1, 10)}\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=random_seed)\n",
    "\n",
    "# Set up GridSearchCV with log-loss as the scoring metric\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_log_loss',  # Use negative log-loss for compatibility with GridSearchCV\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "# Extract results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'max_depth': results['param_max_depth'],\n",
    "    'log_loss_train': -results['mean_train_score'],  # Negate to get positive log-loss\n",
    "    'log_loss_val': -results['mean_test_score']  # Negate to get positive log-loss\n",
    "})\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "plot_data_melted = plot_data.melt(\n",
    "    id_vars='max_depth',\n",
    "    value_vars=['log_loss_train', 'log_loss_val'],\n",
    "    var_name='set',\n",
    "    value_name='log-loss'\n",
    ")\n",
    "\n",
    "# Plot the log-loss for training and validation sets\n",
    "sns.lineplot(x='max_depth', y='log-loss', hue='set', data=plot_data_melted)\n",
    "plt.title('Log-Loss vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Log-Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what is selected to train on we can get very different models. Especially when the model is allowed to overfit to the data (high variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxTo1fsvE2qi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the model\n",
    "cls = DecisionTreeClassifier(max_depth=9)\n",
    "\n",
    "# Initialize DataFrame to store predictions and true labels\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "# List to store per-iteration metrics\n",
    "iteration_metrics = []\n",
    "\n",
    "# Perform multiple train-test splits\n",
    "for i in range(20):\n",
    "    train_X, val_X, train_y, val_y = train_test_split(\n",
    "        train, train_labels, test_size=0.5, random_state=i\n",
    "    )\n",
    "\n",
    "    cls.fit(train_X, train_y)\n",
    "    predictions_val = cls.predict(val_X)\n",
    "    predictions_val_prob = cls.predict_proba(val_X)[:, -1]  # Probabilities for the positive class\n",
    "\n",
    "    # Calculate log-loss and accuracy\n",
    "    iteration_log_loss = log_loss(val_y, predictions_val_prob)\n",
    "    iteration_accuracy = accuracy_score(val_y, predictions_val)\n",
    "    iteration_metrics.append((i, iteration_log_loss, iteration_accuracy))\n",
    "\n",
    "    print(f\"Seed: {i} | Log-Loss: {iteration_log_loss:.6f} | Accuracy: {iteration_accuracy:.6f}\")\n",
    "\n",
    "    # Store the predictions and true labels in a DataFrame\n",
    "    fold_predictions = pd.DataFrame({\n",
    "        'Instance': val_X.index,\n",
    "        f'Seed_{i}': predictions_val_prob,\n",
    "        'True_Label': val_y.values\n",
    "    })\n",
    "\n",
    "    # Merge predictions from each seed\n",
    "    if predictions_df.empty:\n",
    "        predictions_df = fold_predictions\n",
    "    else:\n",
    "        predictions_df = pd.merge(predictions_df, fold_predictions, on=['Instance', 'True_Label'], how='outer')\n",
    "\n",
    "# Calculate the average prediction for each instance\n",
    "predictions_df['Avg_Prediction'] = predictions_df.filter(like='Seed_').mean(axis=1)\n",
    "\n",
    "# Calculate the log-loss based on the averaged predictions\n",
    "average_log_loss = log_loss(predictions_df['True_Label'], predictions_df['Avg_Prediction'])\n",
    "print(f\"\\nAverage Model Log-Loss Across All Seeds: {average_log_loss:.6f}\")\n",
    "\n",
    "# Convert iteration metrics to a DataFrame for further analysis if needed\n",
    "metrics_df = pd.DataFrame(iteration_metrics, columns=['Seed', 'Log_Loss', 'Accuracy'])\n",
    "\n",
    "# Plot the pairplot\n",
    "sns.pairplot(predictions_df.filter(like='Seed_'))\n",
    "\n",
    "# Adjust the layout explicitly\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Pairwise Comparison of Predicted Probabilities Across Seeds\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUgY_6Ply8X3"
   },
   "source": [
    "# 5. Ensemble learning: bagging\n",
    "\n",
    "We have seen that bias and variance play an important role in Machine Learning. \n",
    "\n",
    "Let's first see what bagging can do for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m16JfepBy8X4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=10),random_state=random_seed)\n",
    "                                                            \n",
    "cls.fit(train_X,train_y)\n",
    "\n",
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
    "\n",
    "predictions_train_prob = cls.predict_proba(train_X)\n",
    "predictions_val_prob = cls.predict_proba(val_X)\n",
    "\n",
    "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsA-43z8y8X9"
   },
   "source": [
    "With the `RandomForestClassifier` the variance of the decision tree is reduced also by selecting features for decision tree contruction at random. Let's see how far we get with default hyperparameter values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIfSnbtGy8X-"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cls = RandomForestClassifier(random_state=random_seed)\n",
    "\n",
    "cls.fit(train_X,train_y)\n",
    "\n",
    "predictions_train = cls.predict(train_X)\n",
    "predictions_val = cls.predict(val_X)\n",
    "\n",
    "print(\"Accuracy: (%f) %f\"%(accuracy_score(predictions_train, train_y),accuracy_score(predictions_val, val_y)))\n",
    "\n",
    "predictions_train_prob = cls.predict_proba(train_X)\n",
    "predictions_val_prob = cls.predict_proba(val_X)\n",
    "\n",
    "print(\"Log-loss: (%f) %f\"%(log_loss(train_y,predictions_train_prob[:,1]),log_loss(val_y,predictions_val_prob[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDdxvUZ6ZCd1"
   },
   "source": [
    "Now lets do hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEPUwFoXV2fC"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],    # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]    # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "cls = RandomForestClassifier(random_state=random_seed)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cls,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_log_loss',  # Use negative log-loss as the evaluation metric\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model on training and validation data\n",
    "predictions_train = best_model.predict(train_X)\n",
    "predictions_val = best_model.predict(val_X)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_train = accuracy_score(train_y, predictions_train)\n",
    "accuracy_val = accuracy_score(val_y, predictions_val)\n",
    "print(\"Accuracy: (%f) %f\" % (accuracy_train, accuracy_val))\n",
    "\n",
    "# Predict probabilities\n",
    "predictions_train_prob = best_model.predict_proba(train_X)[:, 1]\n",
    "predictions_val_prob = best_model.predict_proba(val_X)[:, 1]\n",
    "\n",
    "# Log-loss\n",
    "log_loss_train = log_loss(train_y, predictions_train_prob)\n",
    "log_loss_val = log_loss(val_y, predictions_val_prob)\n",
    "print(\"Log-loss: (%f) %f\" % (log_loss_train, log_loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Histone_marks_dt.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
